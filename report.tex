\documentclass[12pt]{report}

% Packages (keep sorted)
\usepackage{
    algorithm,      % For algorithm environment
    algpseudocode,  % For pseudocode
    hyperref,       % For PDF bookmarks
}

% PDF bookmarks setup (keep sorted)
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
}

\begin{document}

% Info section

% TODO(sanjay): come up with something more creative
\title{RTX Project Report}

\author{
    Craig, Shale\\
    20371384\\
    \texttt{sakcraig@uwaterloo.ca}
    \and
    Klen, Alex\\
    20372654\\
    \texttt{ayklen@uwaterloo.ca}
    \and
    Menakuru, Sanjay\\
    20374915\\
    \texttt{smenakur@uwaterloo.ca}
    \and
    Wei, Jonathan\\
    XXXXXXXX\\
    \texttt{XXXX@uwaterloo.ca}
}

\maketitle

\begin{abstract}
    Here is our awesome abstract
\end{abstract}

\tableofcontents
\listofalgorithms

% NOTE(sanjay): uncomment these if we add any figures or tables
% \listoffigures
% \listoftables


\part{Introduction}

\part{Kernel Implementation}
% This part is at max 30 pages.

\chapter{Scheduler}
% Make sure to include pseudocode and testing, if appropriate.

\section{Description}
    The scheduler is responsible for dividing processor time amongst all of the
    system's processes. It handles context switching between processes, deciding
    which process to schedule next, and preempting or blocking processes when
    appropriate. Processes can be in one of the following states: new, ready,
    running, blocked on memory, blocked on message, or unused. `New' is a state
    that processes are given after being initialized so that the scheduler can
    properly context switch to them for the first time. The running state
    signifies that the process is the only currently running process in the
    system. Processes which are ready are placed in a priority queue, and the
    scheduler will select the ready process with the highest priority to run
    next. Processes which are blocked on memory are put into another priority
    queue called the blocked-on-memory queue, and when memory is freed, the
    blocked process with the highest priority is unblocked. Processes that are
    blocked on a message remain blocked until they are sent a message.

    A process can be preempted when it frees memory or sends a message. If the
    process that it unblocks has a higher priority, then the currently running
    process is preempted.

    The ready queues are priority queues implemented using heaps. The decision
    was made to use a priority queue for all ready/blocked processes instead of
    a queue for each priority level because it allowed the system to support an
    arbitrary number of priorities without having the release\_processor runtime
    depend on the number of priorities.

    Pseudocode for process initialization is listed in Algorithm
    \ref{code:proc_init}.

    \begin{algorithm}
        \caption{Process Initialization Pseudocode}
        \label{code:proc_init}
        \begin{algorithmic}[1]
        \Function{k\_initProcesses}{}
            \State Initialize process-ready queue
            \State Initialize blocked-on-memory queue

            \For{each process p, index i}
                \State p.pid $\gets$ i
                \State p.stack $\gets$ \Call{acquireMemoryBlock}{$ $} three times + memory block size \Comment{Use three memory blocks for each process stack and place stack pointer at end of third one}
                \State push initial required values onto stack
                \State set p.startLocation to appropriate process function location
                \Comment{Reserve a debug envelope for each process to be able to display a debug message even if there is no free memory in the system}
                \State p.debugEnvelope $\gets$ acquireMemoryBlock()
                \State p.state $\gets$ new
                \State add p to the process-ready queue
            \EndFor \\
            \State Initialize CRT proc
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    See Algorithm \ref{code:release_processor} for a pseudocode implementation
    for the kernel-level function k\_releaseProcessor. This function is called
    any time the current process should be preempted and a new process should be
    scheduled. The function takes a parameter that specifies what the reason is
    for the context switch. The release reason can be one of the following: a
    voluntary yield, a process priority change, memory was freed, the system is
    out of memory, a message was sent, or a message is being received. The scheduling
    behavior can be different for some of these reasons.

\begin{algorithm}
    \caption{Release Processor Pseudocode}
    \label{code:release_processor}
    \begin{algorithmic}[1]
    \Function{k\_releaseProcessor}{releaseReason}
        \Comment{First we process the changed states of a few interrupt-driven systems here to avoid ISRs modifying kernel state while they are interrupting kernel methods}
        \State \Call{k\_processUartOutput}{$ $}
        \State \Call{k\_processUartInput}{$ $}
        \State \Call{k\_processDelayedMessages}{$ $}

        \Comment{These variables are set depending on the release reason}
        \State define targetState, \Comment{The state to give the currently running process}
            \State \indent srcQueue, \Comment{The queue to schedule the next process 
            from}
            \State \indent dstQueue \Comment{The queue to insert the current process into}

        \If{releaseReason = out of memory}
            \State srcQueue $\gets$ process-ready queue
            \State dstQueue $\gets$ blocked-on-memory queue
            \State targetState $\gets$ blocked on memory
        \ElsIf{releaseReason = message was received}
            \If{the current process's mail queue is empty}
                \State srcQueue $\gets$ process-ready queue
                \State dstQueue $\gets$ NULL \Comment{Don't place in any queue}
                \State targetState $\gets$ blocked on message
            \Else
                \State srcQueue $\gets$ process-ready queue
                \State dstQueue $\gets$ process-ready queue
                \State targetState $\gets$ ready
            \EndIf
        \Else
            \State srcQueue $\gets$ process-ready queue
            \State dstQueue $\gets$ process-ready queue
            \State targetState $\gets$ ready

            \If{currentProcess is the null process}
                \Comment{Keep the null process out of the ready queue - handle it separately}
                \State dstQueue $\gets$ NULL
            \EndIf
        \EndIf

        \If{srcQueue $\ne$ NULL and srcQueue.size $>$ 0}
            \State nextProc $\gets$ \Call{srcQueue.pop}{$ $}
        \Else
            \State nextProc $\gets$ the null process
        \EndIf

    \algstore{releaseProcessor}
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \begin{algorithmic}[1]
    \algrestore{releaseProcessor}

        \If{currentProcess $\ne$ NULL}
            \Comment{Save old process info}
            \State currentProcess.stack $\gets$ \Call{getStackPointer}{$ $}
            \State currentProcess.state $\gets$ targetState
            \If{dstQueue $\ne$ NULL}
                \State \Call{dstQueue.add}{currentProcess}
            \EndIf
        \EndIf

        \State oldState $\gets$ nextProc.state
        \State nextProc.state $\gets$ running
        \State currentProcess $\gets$ nextProc
        \State \Call{setStackPointer}{nextProc.stack}
        \If{oldState = new}
            \State \Call{\_\_rte}{$ $} \Comment{If first time, pop the exception stack frame}
        \EndIf
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\section{Running Time Analysis}

    The runtime for k\_initProcesses is $O(n\log n)$, where n is the number of
    processes in the system. This is because each processes is inserted into the
    process-ready queue, which is implemented as a heap and therefore taking
    $O(\log n)$ to insert an element. This function could be trivially modified
    to achieve a runtime of $O(n)$ time by putting all of the heap elements in
    an array first, and then heapifying it afterwards. Since there is a
    small, fixed number of processes in the system, this small optimization
    wasn't completed since this function only runs once at system startup.

    The runtime for k\_releaseProcessor, with the assumption that the UART and
    delayed message processing at the beginning will typically take $O(1)$ time,
    takes $O(\log n)$ time, where n is the number of processes in the system. It
    is bounded by removing and inserting from the source and destination
    priority queues (heaps). This provides good performance, and we can have an
    arbitrary number of priorities in the system because a priority queue was
    used instead of a queue per priority level.

\chapter{Memory Allocator}
% Make sure to include pseudocode and testing, if appropriate.

\section{Description}
    The memory allocator was designed with two orthogonal layers. The first is
    called the `block layer', and is responsible for allocating and deallocating
    blocks at a low level. The second is called the `metadata layer'; this layer
    builds upon the work done by the block layer, and adds the ability to store
    metadata about blocks. We will describe both of these layers independently.

\subsection{Block Layer}
    The block layer has a conceptually simple role. It is responsible for
    managing a pool of contiguous memory. Given a start memory address, an
    end memory address, and a block size, the block
    layer must provide two pieces of functionality. It must support allocating
    a block from the pool, and it must support freeing a block back into the
    pool. One design constraint is that it must prefer reusing a previously
    allocated block that has been freed, rather than handing out a block that
    has never been allocated before. The reason for this constraint will
    become clear when discussing the metadata layer.

    See Algorithm \ref{code:mem_block} for a pseudocode implementation of this
    layer. Note that this layer returns an error code when it runs out of
    memory. Further, this layer does some rudimentary sanity checking but
    doesn't handle some obvious error conditions. For example, this layer allows
    blocks to be inserted into the free list twice (assuming a trivial linked
    list implementation). This is not an error; it is the responsibility of
    the metadata layer to never allow this to happen. Finally, it is important
    to point out that this layer automatically zeroes memory. This is slightly
    detrimental to performance, but allows user processes to use binary
    comparisons between structs without caring about garbage in the struct's
    padding. We deemed this convenience worth the slight performance overhead.

    \begin{algorithm}
        \caption{Block layer pseudocode}
        \label{code:mem_block}
        \begin{algorithmic}[1]

            \State $blockSize \gets 2^7$
            \Comment{Blocksize in bytes (configurable)}

            \State $startAddr \gets 0$
            \Comment{Start address of pool (inclusive)}

            \State $endAddr \gets 2^{16}$
            \Comment{End address of pool (exclusive)}

            \State $nextAddr \gets startAddr$
            \Comment{Next available address in pool}

            \State $freeList \gets \{\}$
            \Comment{List of freed blocks}\\

            \Function{alloc\_block}{$ $}
                \If {$|freeList| > 0$} \Comment{Check free-list}
                    \State $blk \gets freeList[0]$
                    \State $freeList = freeList - \{blk\}$
                    \State \Call{zero}{$blk$}
                    \State \Return $blk$
                \EndIf\\

                \If {$nextAddr + blockSize >= endAddr$}
                    \Comment{Out-of-memory}
                    \State \Return $-1$
                \EndIf\\

                \State $blk \gets nextAddr$
                \State $nextAddr \gets nextAddr + blockSize$
                \State \Call{zero}{$blk$}
                \State \Return $blk$
            \EndFunction\\
            \Function{free\_block}{$blk$}
                \If {$blk < startAddr \vee nextAddr <= blk \vee endAddr <= blk$}
                    \State \Return \Comment{Outside valid range}
                \EndIf\\

                \If {$blk - startAddr \not\equiv 0 \bmod{blockSize}$}
                    \State \Return \Comment{Unaligned}
                \EndIf\\

                \State $freeList = freeList \cup \{blk\}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}


\subsection{Metadata Layer}
    The metadata layer builds atop the block layer, and adds the ability for the
    allocator to store metadata about blocks. Since it is built upon the block
    layer, it must fit its metadata storage in blocks. To avoid a costly setup
    phase, the metadata layer uses a lazy, arena-based storage scheme. The
    general version of the metadata layer supports an arbitrary amount of
    metadata per block ranging from some non-zero amount of metadata to half the
    block size worth of metadata.

    For the sake of description, let us call the number of metadata fields that
    fit inside a block $k$. That is to say, one block can hold the metadata for
    $k$ blocks. The metadata layer discriminates between two types of blocks,
    data blocks and arena header blocks. The metadata layer implements a
    function that takes a block and returns its associated arena header block.
    For our purposes, this function acts as the identity function when passed
    an arena header block. These definitions lead to the notion that an arena
    header block is responsible for storing the metadata of itself, as well as
    the following $k-1$ blocks. The metadata layer is responsible for serving
    the user-facing API.

    See Algorithm \ref{code:mem_meta} for a pseudocode implementation of this
    layer. Note that this layer can also access the variables defined by the
    block layer (they can be found in Algorithm \ref{code:mem_block}). Also note that we have elided the code of several trivial helper functions. For
    example, we did not specify the body of the `get\_header' function. Since
    the implementation of this function was some simple modular arithmetic, we
    did not feel it was worth wasting the reader's time with such minutiae.

    While the metadata layer is parameterized on $k$, for our actual
    implementation, we chose $k$ to be equal to our block size; this implies
    the metadata for each process fits inside 1 byte. Indeed, we stored the
    owner pid of each block as the only metadata, and the bitwidth of the pid
    type in our system was 8.

    \begin{algorithm}
        \caption{Metadata layer pseudocode}
        \label{code:mem_meta}
        \begin{algorithmic}[1]

            \State $k \gets 2^7$
            \Comment{Blocks per arena (configurable)}

            \State $arenaSize \gets k * blockSize$
            \Comment{Arena size (in bytes)}

            \Function{request\_memory\_block}{$pid$}
                \State $header \gets -1$
                \State $blk \gets -1$ \\

                \State $blk \gets \Call{alloc\_block}{ }$
                \If {$blk = -1$}
                    \State \Return $-1$
                \EndIf\\

                \State $header \gets \Call{get\_header}{blk}$
                \If {$blk = header$}
                    \State $blk \gets \Call{alloc\_block}{ }$
                \EndIf\\

                \If {$blk = -1$}
                    \State \Return $-1$
                \EndIf\\

                \State \Call{set\_owner}{$header, blk, pid$}
                \State \Return $blk$
            \EndFunction\\
            \Function{free\_memory\_block}{$blk, pid$}
                \State $header \gets \Call{get\_header}{blk}$
                \If {$header = -1$}
                    \State \Return $-1$
                \EndIf\\

                \If {$ \neg \Call{is\_owner}{header, blk, pid}$}
                    \State \Return $-1$
                \EndIf\\

                \State \Call{set\_owner}{$header, blk, 0$}
                \State \Call{free\_block}{$blk$}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}


\section{Theoretical Analysis}
    Both layers were carefully designed to be $O(1)$ for all operations. This
    should be trivial by inspection, as all the functions do a tiny amount of
    arithmetic, and considering we used a linked-list as the implementation of
    the free list.

    A slightly more interesting analysis is the space overhead of our metadata
    scheme. The metadata layer uses $\frac{k-1}{k}$ blocks for storing user
    data; this yields a storage overhead of $\frac{1}{k}$. For our
    implementation, we picked $k=2^7$, yielding an overhead of $0.78\%$, or a
    utilization rate of $99.22\%$.

\section{Measurements}
    % TODO(sanjay): do some measurements

\chapter{Message Passing}
% Make sure to include pseudocode and testing, if appropriate.

\section{Description}
    This micro-kernel supports three user API functions for message passing: send\_message, 
    receive\_message, and send\_delayed\_message. Each function requires the user process to use a special data structure called Envelope, which is available in a provided header file. Following is a table of the fields available on the Envelope data structure.

    \begin{tabular}{l | l}
        Type & Name \\
        \hline
        uint32\_t & messageType \\
        char[MESSAGEDATA\_SIZE\_BYTES] & messageData \\
        ProcId & srcPid \\
        ProcId & dstPid \\
        uint32\_t & sendTime \\
        Envelope* & next \\
    \end{tabular}

    To obtain an envelope, a user process must acquire a memory block and then
    cast  it to the Envelope type. When sending a message, the user should set
    the messageType and messageData fields to values they wish to send. The
    srcPid and dstPid fields may be filled out, but the kernel will overwrite
    them to their correct values. The rest of the fields are reserved for kernel
    use and they will be overwritten after the user sends the message.  After
    sending an envelope, the user process no longer owns that block of memory
    and they should clear all references to it.

    The kernel maintains a priority queue of delayed messages (where priority is
    the time it should be delivered) for the system and a message queue for each
    process. A function called k\_processDelayedMessages (see Algorithm
    \ref{code:process_delayed_messages}) is called in release\_processor, and it
    delivers any delayed messages that have waited for their specified delay.
    The  message queue for each process is where messages are stored when their
    delivered  to a process that is not blocked on message.

    Sending a message may cause the sending process to be preempted. This will
    occur if the receiving process is blocked on receiving a message and it has
    a higher priority. Calling receive\_message will cause the process to block
    on message, unless  it has messages in its message queue. Sending a delayed
    message, however, will never cause  the sender to block. See Algorithm
    \ref{code:send_message} for a pseudocode implementation  of send\_message,
    Algorithm \ref{code:receive_message} for receive\_message, and
    \ref{code:send_delayed_message} for send\_delayed\_message.

    \begin{algorithm}
    \caption{Send Message}
    \label{code:send_message}
    \begin{algorithmic}[1]
        \Function{k\_sendMessage}{envelope, srcPid, dstPid}
            \State Initialize reserved kernel fields of envelope to a zeroed state
            \State envelope.srcPid = srcPid
            \State envelope.dstPid = dstPid

            \If{srcPid or dstPid is invalid}
                \State \Return error code
            \EndIf
            \State dstPCB $\gets$ PCB of process with pid dstPid
            \State srcPCB $\gets$ PCB of process with pid srcPid

            \If{srcPid owns memory block envelope}
                \State Set owner of envelope to dstPid
            \Else
                \State \Return error code
            \EndIf

            \State Add envelope to dstPCB's message queue

            \If{dstPCB.state = blocked on message} \Comment{Unblock receiver}
                \State dstPCB.state $\gets$ ready
                \State Add dstPCB to process-ready queue
                \If{dstPCB.priority $<$ currentProcess.priority}
                    \State \Call{k\_releaseProcessor}{$ $} \Comment{Preempt if unblocked process has higher priority}
                \EndIf
            \EndIf
        \EndFunction
    \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
    \caption{Receive Message}
    \label{code:receive_message}
    \begin{algorithmic}[1]
        \Function{k\_receiveMessage}{}
            \State message $\gets$ \Call{currentProc.messageQueue.top}{$ $}
            \While{message = NULL} \Comment{Loop until message exists}
                \State \Call{k\_releaseProcessor}{message being received} \Comment{Blocks 
                this process}
                \State message $\gets$ \Call{currentProc.messageQueue.top}{$ $}
            \EndWhile

            \State \Call{currentProc.messageQueue.pop}{$ $}

            \State message.next $\gets$ NULL
            \Comment{Clear next pointer so user doesn't have next message}

            \State Set owner of message to dstPid
            \State \Return message
        \EndFunction
    \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
    \caption{Send Delayed Message}
    \label{code:send_delayed_message}
    \begin{algorithmic}[1]
        \Function{k\_sendDelayedMessage}{envelope, srcPid, dstPid, delay}
            \State Initialize reserved kernel fields of envelope to a zeroed state
            \If{srcPid or dstPid is invalid}
                \State \Return error code
            \EndIf
            \If{srcPid owns memory block envelope}
                \State Set owner of envelope to kernel
            \Else
                \State \Return error code
            \EndIf

            \State envelope.sendTime $\gets$ \Call{k\_getTime}{$ $} + delay
            \Comment{Set the time the envelope will be delivered}
            \State envelope.srcPid $\gets$ srcPid
            \State envelope.dstPid $\gets$ dstPid

            \State Add envelope to kernel delayed message queue
        \EndFunction
    \end{algorithmic}
    \end{algorithm}

    \begin{algorithm}
    \caption{Process Delayed Messages}
    \label{code:process_delayed_messages}
    \begin{algorithmic}[1]
        \Function{k\_processDelayedMessages}{}
            \State messageQueue $\gets$ kernel delayed message queue
            \State currentTime $\gets$ \Call{k\_getTime}{$ $}

            \If{messageQueue is empty}
                \State \Return
            \EndIf

            \Loop
                \State message $\gets$ \Call{messageQueue.top}{$ $}

                \If{message = NULL or message.sendTime $>$ currentTime}
                    \State \Return
                    \Comment{Loop until there are no more messages to deliver}
                \EndIf

                \State \Call{messageQueue.pop}{$ $}
                \State Set owner of message to message.srcPid
                \Comment{Set to srcPid so that we can use k\_sendMessage}
                \State \Call{k\_sendMessage}{message, message.srcPid, message.dstPid}
            \EndLoop
        \EndFunction
    \end{algorithmic}
    \end{algorithm}

\section{Running Time Analysis}

    The runtime of k\_sendMessage (assuming process is not preempted) is $O(1)$
    because all operations take constant time. Adding a message to the receiving
    process's message queue  takes constant time because it is simply a linked-
    list. The runtime of k\_receiveMessage (assuming there is a message to
    receive)  is also $O(1)$ because it only looks at the message at the head of
    the queue.

    The runtime of k\_sendDelayedMessage is bounded by the time it takes to add
    a  message to the kernel delayed message queue. This is a priority queue
    implemented  as a heap, so it takes $O(\log n)$ time for insertion. This is
    a good runtime  for sending a delayed message, considering that delayed
    messages are not necessarily delivered in the order they are sent. A heap
    efficiently allows retrieval of the earliest message without having to sort
    all of the messages.

    The runtime of k\_processDelayedMessages depends upon how many delayed
    messages  need to be delivered at the same time. Each message that needs to
    be delivered  is removed from the kernel delayed message queue, which is
    implemented  with a heap. This makes  the runtime $(m \log n)$, where $m$ is
    the number of messages that need to be sent immediately, and $n$ is the
    number of delayed messages currently waiting in the system. This runtime is
    reasonable because it is very unlikely that a large number of messages need
    to be delivered at exactly the same time. It is possible to improve this
    runtime by using a skip list. This would allow binary searching for the last
    message in the queue that needs to be sent, and then linearly removing and
    delivering all messages  from the head of the list to the found message.
    This optimization would result  in worst-case $O(\log n + m)$ time. This
    optimization wasn't done because the  small likelihood of m being large
    didn't justify a much more complex implementation.

\section{Measurements}
    


\chapter{I/O}
% Make sure to include pseudocode and testing, if appropriate.

\section{UART output}

\section{UART input}

\chapter{Misc}
% Make sure to include pseudocode and testing, if appropriate.

\section{Bridge Layer}

\part{User-level Processes}

\chapter{Proc 1}

\chapter{Proc 2}

\part{Lessons Learned}
% They call this the lessons learned summary.
% 1-2 pages
% what you did do well, both technically and organizationally, and what you
% would do differently if you were to do it again

\chapter{Appendix A - Measurements}

% \begin{table}{h}
% \centering
\begin{tabular}{l | l | l | l | l}
    Function & Trial & Time ($\mu s$) & \# of Calls & Average time / call ($\mu s$) \\
    \hline
    k\_sendMessage&1&601.58&552&1.090 \\
    k\_receiveMessage&1&408.22&565&0.723 \\
    k\_aquireMemoryBlock&1&244.12&294&0.830 \\
    k\_sendMessage&2&647.44&594&1.090 \\
    k\_receiveMessage&2&437.78&606&0.722 \\
    k\_aquireMemoryBlock&2&258.68&320&0.808 \\
    k\_sendMessage&3&630.99&579&1.090 \\
    k\_receiveMessage&3&426.83&591&0.722 \\
    k\_aquireMemoryBlock&3&259.24&321&0.808 \\
    k\_sendMessage&4&108.80&100&1.088 \\
    k\_receiveMessage&4&74.44&110&0.677 \\
    k\_aquireMemoryBlock&4&92.47&123&0.752 \\
    k\_sendMessage&5&750.63&687&1.093 \\
    k\_receiveMessage&5&497.09&693&0.717 \\
    k\_aquireMemoryBlock&5&329.90&447&0.738 \\


\end{tabular}
% \end{table}

\end{document}
